---
title: "Final"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Carregamento da Base

O primeiro passo para iniciar a análise foi carregar as duas bases e salvar certos atributos como variáveis separadas. Isso nos permitiu preparar o chão para facilitar a análise mais para frente.

```{r}
trainData <- read.csv("data/Training set values - 4910797b-ee55-40a7-8668-10efd5c1b960.csv", sep=",")
trainData <- trainData[order(trainData$id),]
trainLabels <- read.csv("data/Training set labels - 0bf8bc6e-30d0-4c50-956a-603fc693d966.csv", sep=",")
trainLabels <- trainLabels[order(trainLabels$id),]
trainDataset <- cbind(trainData, status_group = trainLabels$status_group)

testData <- read.csv("data/Test set values  - 702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv", sep=",")
submissionFormat <- read.csv("data/SubmissionFormat.csv", sep=",")
testData <- cbind(testData, status_group = submissionFormat$status_group)
```

## Preparação Inicial da Base

Iniciamos nossa análise fazendo uma análise manual prévia da base. Nela, usamos a explicação dos atributos presentes no site do desafio [Pump it Up: Descrição dos Atributos do Dataset](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/25/). A partir disso, conseguimos determinar quais seriam os atributos mais importantes para nossa análise e já eliminar alguns deles.

O próximo passo foi analisar os dados do Dataset em si. A partir disso, conseguimos determinar que vários deles estavam zerados ou com dados inconsistentes e já conseguimos excluir-los.

Por último, excluímos atributos que continham muitos fatores diferentes e não conseguiriam ser
levados em conta na análise, e um dos atributos que era idêntico à outro.

Reconhecemos que, dentre os atributos excluídos, estavam alguns que poderiam ser relevantes, tal como a data da coleta do dado. No entanto, acreditamos que eles estariam mais presentes em análises mais completas da base, por isso continuamos com a decisão de os excluir.

Fizemos também uma exclusão de algumas linhas com atributos importantes que estavam zerados ou com dados esdruxulos. Desistimos de continuar com isso, entretanto, visto que, para submeter uma base para o desafio, ela necessita conter o mesmo número de linhas da base original (por esse motivo, na função abaixo, colocamos uma opção de "training" que nos permite escolher se vamos excluir as linhas ou não).

```{r}
cleanData <- function(dataset, training = FALSE){
  
  # Meaningless columns
  dataset <- dataset[ , -which(names(dataset) %in% 
                                 c('id',
                                   'num_private', 
                                   'wpt_name', 
                                   'recorded_by', 
                                   'date_recorded', 
                                   'funder',
                                   'installer',
                                   'scheme_name',
                                   'public_meeting', 
                                   'scheme_management'
                                   ))]
  
  # More than 53 factors
  dataset <- dataset[ , -which(names(dataset) %in% c('subvillage','lga','ward'))]
  
  # Removing repeated columns => identical()
  dataset <- dataset[ , -which(names(dataset) %in% c('payment_type', 'quantity_group'))]
  
  # Removing columns that mean the same thing
  dataset <- dataset[ , -which(names(dataset) %in% 
                                 c('region_code', 
                                   'source_class',
                                   'waterpoint_type_group',
                                   'extraction_type_group',
                                   'extraction_type_class', 
                                   'management_group'
                                   ))] # region_code means the same as region
  
  # There are many related fields, but the level of specificity required will be determined as we test
  # different algorithms
  if(training == TRUE) {
    # Removing meaningless rows
    dataset <- dataset[dataset$population > 1,]
    dataset <- dataset[complete.cases(dataset), ]
  }
  return(dataset)
}
```

## Preparação para o treino

Iniciamos a preparação para realizar o treinamento criando as variáveis trainDataset e testData utilizando a função acima.

```{r}
trainDataset <- cleanData(trainDataset, training = TRUE)
testData <- cleanData(testData)
```

Depois disso, realizamos o carregamento da biblioteca RandomForest que será usada para fazer a predição. Definimos também os valores de ntree, mtry e nodesize para serem usados no gridSearch mais abaixo.

```{r}
library(randomForest)
testData <- rbind(trainDataset[1, ] , testData)
testData <- testData[-1,]


ntree <- c(500,1000, 1200)
mtry <- c(5,9,12)
nodesize <- c(3, 5, 7)
#maxnodes <- c(10,100,NA)

formula <- status_group ~ .
result <- data.frame(mtry=integer(), ntree=integer(), nodesize=integer(),  oob=integer())
```

A partir da criação das variáveis, é possível fazermos o GridSearch.

```{r}
for(i in mtry){
  for(j in ntree){
    for(k in nodesize){
      print(paste(i,'  ',j,'  ',k))
      model <- randomForest(formula, data=trainDataset, mtry = i, ntree = j, nodesize = k, na.action=na.roughfix)  
      result <- rbind(result, c(i,j,k,model$err.rate[nrow(model$err.rate),1]))
    }
  }
}
names(result) <- c('mtry','ntree','nodesize','oob')
View(result)
```

Depois de rodar GS, rodamos o randomForest com os parâmetros que identificamos ser os de melhor resultado durante o GS. Assim, fazemos o predict usando essa configuração e fazemos um plot do modelo.

```{r}
model <- randomForest(formula, data=trainDataset, do.trace=100, mtry = 3, ntree=800, nodesize=2)
pred <- predict(model, newdata = testData)

plot(model)
```

O último passo é simplesmente concatenar os dados do pred com o submissionFormat para termos a tabela no formato correto para submissão e exportar o csv.

```{r}
submissionFormat$status_group <- pred
write.csv(submissionFormat, "Submission.csv", row.names = FALSE)
```
